{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä 02 ‚Äî Data Pipeline Validation\n",
    "\n",
    "This notebook validates the complete data pipeline for the **ModernBERT-RGAT** project:\n",
    "\n",
    "1. **Config loading** ‚Äî verify `configs/config.yaml`\n",
    "2. **Data loading** ‚Äî load all 3 processed CSV datasets\n",
    "3. **Stratified splitting** ‚Äî 80/10/10 at sentence level\n",
    "4. **Leakage checks** ‚Äî ensure no sentence overlap across splits\n",
    "5. **Distribution analysis** ‚Äî verify stratification preserves polarity ratios\n",
    "6. **Class weights** ‚Äî compute inverse-frequency weights for imbalanced labels\n",
    "7. **Save verified splits** ‚Äî cache for downstream training\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Ensure project root is on path\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.data_pipeline import (\n",
    "    load_config,\n",
    "    load_dataset,\n",
    "    load_all_datasets,\n",
    "    stratified_sentence_split,\n",
    "    validate_split,\n",
    "    print_split_summary,\n",
    "    compute_class_weights,\n",
    "    build_splits,\n",
    "    build_all_splits,\n",
    ")\n",
    "\n",
    "# Style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('deep')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(\"configs/config.yaml\")\n",
    "\n",
    "print(\"üìã Configuration Summary:\")\n",
    "print(f\"  Model backbone:    {config['model']['backbone']}\")\n",
    "print(f\"  Max sequence len:  {config['model']['max_len']}\")\n",
    "print(f\"  Batch size:        {config['training']['batch_size']}\")\n",
    "print(f\"  Learning rate:     {config['training']['learning_rate']}\")\n",
    "print(f\"  Epochs:            {config['training']['epochs']}\")\n",
    "print(f\"  Split ratios:      {config['data']['split']['train']}/{config['data']['split']['val']}/{config['data']['split']['test']}\")\n",
    "print(f\"  Random seed:       {config['data']['split']['seed']}\")\n",
    "print(f\"  Polarity labels:   {config['labels']['polarity']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Load Processed Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìÇ Loading all processed datasets...\\n\")\n",
    "datasets = load_all_datasets(config)\n",
    "\n",
    "for year, df in datasets.items():\n",
    "    print(f\"\\n--- {year} ---\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    print(f\"  Shape:   {df.shape}\")\n",
    "    print(f\"  Polarity values: {df['polarity'].unique().tolist()}\")\n",
    "    implicit = (df['aspect'] == config['data']['implicit_aspect_token']).sum()\n",
    "    print(f\"  Implicit aspects: {implicit}\")\n",
    "    display(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Build Stratified Splits (80 / 10 / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÄ Building stratified sentence-level splits...\\n\")\n",
    "\n",
    "all_splits = build_all_splits(config, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Visual Verification: Polarity Distribution Across Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (year, (train_df, val_df, test_df)) in enumerate(all_splits.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Compute distributions\n",
    "    polarities = sorted(train_df['polarity'].unique())\n",
    "    x = np.arange(len(polarities))\n",
    "    width = 0.25\n",
    "    \n",
    "    train_pcts = [train_df['polarity'].value_counts(normalize=True).get(p, 0)*100 for p in polarities]\n",
    "    val_pcts   = [val_df['polarity'].value_counts(normalize=True).get(p, 0)*100   for p in polarities]\n",
    "    test_pcts  = [test_df['polarity'].value_counts(normalize=True).get(p, 0)*100  for p in polarities]\n",
    "    \n",
    "    bars1 = ax.bar(x - width, train_pcts, width, label='Train', color='#2196F3', alpha=0.85)\n",
    "    bars2 = ax.bar(x,         val_pcts,   width, label='Val',   color='#FF9800', alpha=0.85)\n",
    "    bars3 = ax.bar(x + width, test_pcts,  width, label='Test',  color='#4CAF50', alpha=0.85)\n",
    "    \n",
    "    ax.set_xlabel('Polarity', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Percentage (%)', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'SemEval {year}', fontsize=13, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(polarities, rotation=15)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.set_ylim(0, 80)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bars in [bars1, bars2, bars3]:\n",
    "        for bar in bars:\n",
    "            h = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., h + 0.5,\n",
    "                    f'{h:.1f}', ha='center', va='bottom', fontsize=7)\n",
    "\n",
    "fig.suptitle('Polarity Distribution Across Splits (Stratification Verification)',\n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ If bars within each dataset are approximately equal height per polarity,\")\n",
    "print(\"   stratification is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Data Leakage Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîí Checking for data leakage (sentence overlap)...\\n\")\n",
    "\n",
    "all_clear = True\n",
    "for year, (train_df, val_df, test_df) in all_splits.items():\n",
    "    train_sids = set(train_df['sentence_id'].unique())\n",
    "    val_sids   = set(val_df['sentence_id'].unique())\n",
    "    test_sids  = set(test_df['sentence_id'].unique())\n",
    "    \n",
    "    overlap_tv = train_sids & val_sids\n",
    "    overlap_tt = train_sids & test_sids\n",
    "    overlap_vt = val_sids & test_sids\n",
    "    \n",
    "    total_overlap = len(overlap_tv) + len(overlap_tt) + len(overlap_vt)\n",
    "    \n",
    "    if total_overlap == 0:\n",
    "        print(f\"  ‚úÖ SemEval {year}: No leakage detected\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå SemEval {year}: {total_overlap} overlapping sentences FOUND!\")\n",
    "        all_clear = False\n",
    "\n",
    "print()\n",
    "if all_clear:\n",
    "    print(\"üéâ ALL DATASETS PASS ‚Äî No data leakage across any split!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è LEAKAGE DETECTED ‚Äî Fix the splitting logic before proceeding!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Class Weights for Imbalanced Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öñÔ∏è Computing class weights (inverse frequency) from TRAINING sets...\\n\")\n",
    "\n",
    "label_map = config['labels']['polarity']\n",
    "\n",
    "for year, (train_df, _, _) in all_splits.items():\n",
    "    weights = compute_class_weights(train_df, label_map)\n",
    "    \n",
    "    print(f\"\\n  SemEval {year}:\")\n",
    "    print(f\"  {'Label':12s} {'Count':>6s} {'Weight':>8s}\")\n",
    "    print(f\"  {'‚îÄ'*30}\")\n",
    "    \n",
    "    counts = train_df['polarity'].value_counts()\n",
    "    for label_name, label_idx in sorted(label_map.items(), key=lambda x: x[1]):\n",
    "        cnt = counts.get(label_name, 0)\n",
    "        wt = weights[label_idx]\n",
    "        print(f\"  {label_name:12s} {cnt:6d} {wt:8.4f}\")\n",
    "\n",
    "print(\"\\nüí° Higher weight = rarer class = model pays more attention to it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Split Size Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = []\n",
    "\n",
    "for year, (train_df, val_df, test_df) in all_splits.items():\n",
    "    summary_data.append({\n",
    "        'Dataset': f'SemEval {year}',\n",
    "        'Train Rows': len(train_df),\n",
    "        'Train Sentences': train_df['sentence_id'].nunique(),\n",
    "        'Val Rows': len(val_df),\n",
    "        'Val Sentences': val_df['sentence_id'].nunique(),\n",
    "        'Test Rows': len(test_df),\n",
    "        'Test Sentences': test_df['sentence_id'].nunique(),\n",
    "        'Total Rows': len(train_df) + len(val_df) + len(test_df),\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "display(summary_df)\n",
    "\n",
    "print(\"\\n‚úÖ Phase 2: Data Pipeline ‚Äî COMPLETE\")\n",
    "print(\"\\nüöÄ Ready for Phase 3: Model Architecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Phase 2 Summary\n",
    "\n",
    "| Item | Status |\n",
    "|------|--------|\n",
    "| Config loaded | ‚úÖ |\n",
    "| 3 datasets loaded | ‚úÖ |\n",
    "| Stratified 80/10/10 split | ‚úÖ |\n",
    "| Sentence-level (no leakage) | ‚úÖ |\n",
    "| Distribution preserved | ‚úÖ |\n",
    "| Class weights computed | ‚úÖ |\n",
    "| Splits cached | ‚úÖ |\n",
    "\n",
    "**Next step ‚Üí** Phase 3: Model Architecture"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
